{
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "QYg_pZ9NuHkY"
      },
      "cell_type": "markdown",
      "source": "# Customer Classification using E-Commerce Dataset\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure ML Workspace"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Set up your development environment\nAll the setup for development work can be accomplished in a Python notebook. Setup includes:\n\n- Importing Python packages\n- Connecting to a workspace to enable communication between your local computer and remote resources\n- Creating an experiment to track all your runs\n- Creating a remote compute target to use for training"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Import packages\nImport Python packages you need in this session. Also display the Azure Machine Learning SDK version:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Azure ML SDK Version:  1.0.17\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Connect to a workspace\nCreate a workspace object from the existing workspace."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Warning: Falling back to use azure cli login credentials.\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\nPlease refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/config.json\nPerforming interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FTKLDS448 to authenticate.\nInteractive authentication successfully completed.\necommerce-ws\teastus\tecommerce-aml\teastus\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create an experiment\nCreate an experiment to track the runs in your workspace. A workspace can have multiple experiments:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'final_experiment'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create or attach an existing compute resource\nBy using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\n# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n     # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "creating a new compute target...\nCreating\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-03-26T09:05:57.422000+00:00', 'creationTime': '2019-03-26T09:05:54.230893+00:00', 'currentNodeCount': 0, 'errors': None, 'modifiedTime': '2019-03-26T09:06:10.349008+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 0, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Explore data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Upload data to the cloud\nNow make the data accessible remotely by uploading that data from your local machine into Azure. Then it can be accessed for remote training. The datastore is a convenient construct associated with your workspace for you to upload or download data. You can also interact with it from your remote compute targets. It's backed by an Azure Blob storage account."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name, ds.name)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "AzureBlob ecommercews3975002265 azureml-blobstore-99335e38-05f7-46cc-8735-3c57faeb0b5f workspaceblobstore\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_folder = os.path.join(os.getcwd(), 'Data')\nos.makedirs(data_folder, exist_ok = True)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds.upload(src_dir=data_folder, target_path='Cloud_Data', overwrite=True, show_progress=True)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Uploading /home/nbuser/library/Data/data.csv\nUploaded /home/nbuser/library/Data/data.csv, 1 files out of an estimated total of 1\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_907391106fc645e1aa2532ebd6d709fa"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train on a remote cluster\nFor this task, submit the job to the remote training cluster you set up earlier. To submit a job you:\n\n- Create a directory\n- Create a training script\n- Create an estimator object\n- Submit the job"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create a directory\nCreate a directory to deliver the necessary code from your computer to the remote resource."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = os.path.join(os.getcwd(), \"Train\")\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create a training script\ntrain.ipynb"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.path('Cloud_Data').as_mount()\n}\n\nest = Estimator(source_directory=script_folder,\n                compute_target=compute_target,\n                script_params=script_params,\n                entry_script='train.py',\n                conda_packages=[\"scikit-learn\", \"matplotlib\", \"pandas\", \"numpy\", \"seaborn\", \"nltk\", \"jinja2\"])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(config=est)\nrun",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>final_experiment</td><td>final_experiment_1553593109_bdfad989</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/22ca3877-d6ed-4eaa-94e7-b636d1078dbd/resourceGroups/ecommerce-aml/providers/Microsoft.MachineLearningServices/workspaces/ecommerce-ws/experiments/final_experiment/runs/final_experiment_1553593109_bdfad989\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: final_experiment,\nId: final_experiment_1553593109_bdfad989,\nType: azureml.scriptrun,\nStatus: Queued)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70662f9213664b8082ca4363568445bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=False)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "{'runId': 'final_experiment_1553593109_bdfad989',\n 'target': 'cpucluster',\n 'status': 'Completed',\n 'startTimeUtc': '2019-03-26T09:44:13.690549Z',\n 'endTimeUtc': '2019-03-26T09:47:32.289081Z',\n 'properties': {'azureml.runsource': 'experiment',\n  'ContentSnapshotId': '9b7cf02d-b0db-4526-9c92-93d615286055'},\n 'runDefinition': {'script': 'train.py',\n  'arguments': ['--data-folder',\n   '$AZUREML_DATAREFERENCE_a16b05518ab649c6bf6e652278416d21'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'cpucluster',\n  'dataReferences': {'a16b05518ab649c6bf6e652278416d21': {'dataStoreName': 'workspaceblobstore',\n    'mode': 'Mount',\n    'pathOnDataStore': 'Cloud_Data',\n    'pathOnCompute': None,\n    'overwrite': False}},\n  'jobName': None,\n  'autoPrepareEnvironment': True,\n  'maxRunDurationSeconds': None,\n  'nodeCount': 1,\n  'environment': {'name': 'Experiment final_experiment Environment',\n   'version': 1,\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'name': 'project_environment',\n     'dependencies': ['python=3.6.2',\n      {'pip': ['azureml-defaults']},\n      'scikit-learn',\n      'matplotlib',\n      'pandas',\n      'numpy',\n      'seaborn',\n      'nltk',\n      'jinja2']},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:0.2.2',\n    'enabled': True,\n    'sharedVolumes': True,\n    'preparation': None,\n    'gpuSupport': False,\n    'shmSize': '1g',\n    'arguments': [],\n    'baseImageRegistry': {'address': None,\n     'username': None,\n     'password': None}},\n   'spark': {'repositories': ['https://mmlspark.azureedge.net/maven'],\n    'packages': [{'group': 'com.microsoft.ml.spark',\n      'artifact': 'mmlspark_2.11',\n      'version': '0.12'}],\n    'precachePackages': True}},\n  'history': {'outputCollection': True, 'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'vmPriority': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': 1},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n  'exposedPorts': None},\n 'logFiles': {'azureml-logs/60_control_log.txt': 'https://ecommercews3975002265.blob.core.windows.net/azureml/ExperimentRun/dcid.final_experiment_1553593109_bdfad989/azureml-logs/60_control_log.txt?sv=2018-03-28&sr=b&sig=Uucn2f7NtoP7Nf9lSb8qj3fy5PhAJNOFYJCHTkNIQSc%3D&st=2019-03-26T09%3A56%3A38Z&se=2019-03-26T18%3A06%3A38Z&sp=r',\n  'azureml-logs/80_driver_log.txt': 'https://ecommercews3975002265.blob.core.windows.net/azureml/ExperimentRun/dcid.final_experiment_1553593109_bdfad989/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=JIScVaSjn2P1Q0uTIuwwaRETQ5AkiMqxVolZWGyiklI%3D&st=2019-03-26T09%3A56%3A38Z&se=2019-03-26T18%3A06%3A38Z&sp=r',\n  'azureml-logs/azureml.log': 'https://ecommercews3975002265.blob.core.windows.net/azureml/ExperimentRun/dcid.final_experiment_1553593109_bdfad989/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=aeELMd9zNJFilO7GZUOSIjyysaocyn4p3ilut4kCyjE%3D&st=2019-03-26T09%3A56%3A38Z&se=2019-03-26T18%3A06%3A38Z&sp=r',\n  'azureml-logs/55_batchai_execution.txt': 'https://ecommercews3975002265.blob.core.windows.net/azureml/ExperimentRun/dcid.final_experiment_1553593109_bdfad989/azureml-logs/55_batchai_execution.txt?sv=2018-03-28&sr=b&sig=pGiUorz1jbZvpyfhl%2BRIuxd0kAVLpMj8jUgrw5eYu%2Bw%3D&st=2019-03-26T09%3A56%3A38Z&se=2019-03-26T18%3A06%3A38Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_metrics())\nprint(run.get_file_names())",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'Accuracy': 0.9143538008178975}\n['azureml-logs/55_batchai_execution.txt', 'azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'azureml-logs/azureml.log', 'outputs/test_model.pkl']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# register model \nmodel = run.register_model(model_name='test_model', model_path='outputs/test_model.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "test_model\ttest_model:1\t1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "compute_target.delete()",
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ecommerce.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}